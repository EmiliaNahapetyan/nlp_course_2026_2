{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-21T11:40:49.864579Z",
     "start_time": "2026-02-21T11:40:41.930691Z"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:40:49.895156Z",
     "start_time": "2026-02-21T11:40:49.888197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 100\n",
    "batch_size = 128\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "epochs = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "min_freq = 1\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "f1f4da3e5b29790c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:41:04.457596Z",
     "start_time": "2026-02-21T11:40:50.166228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_text(url):\n",
    "    try:\n",
    "        print(\"Downloading text...\")\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(\"Download failed:\", e)\n",
    "        return None\n",
    "\n",
    "gutenberg_txt_url = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
    "\n",
    "raw = download_text(gutenberg_txt_url)\n",
    "\n",
    "if raw is None:\n",
    "    print(\"Using fallback sample text.\")\n",
    "    raw = (\n",
    "        \"In the quiet village the sun rose slowly. Children ran across fields, \"\n",
    "        \"the baker opened his shop, and a bell rang in the distance. People spoke \"\n",
    "        \"of ordinary things, and the day seemed to promise little more than calm.\"\n",
    "        \"But small events have a way of changing the course of ordinary days.\"\n",
    "        * 50\n",
    "    )\n",
    "\n",
    "print(\"Loaded text length (chars):\", len(raw))"
   ],
   "id": "2602d9771a8b9f06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading text...\n",
      "Loaded text length (chars): 446544\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:41:04.521517Z",
     "start_time": "2026-02-21T11:41:04.464411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def clean_and_tokenize(text):\n",
    "    txt = text\n",
    "    txt = txt.replace(\"\\r\\n\", \"\\n\")\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt)  # normalize whitespace\n",
    "    txt = txt.strip()\n",
    "    # lowercase\n",
    "    txt = txt.lower()\n",
    "    # simple tokenization on whitespace and punctuation as separate tokens\n",
    "    tokens = re.findall(r\"\\w+|[^\\s\\w]\", txt, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "tokens = clean_and_tokenize(raw)\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "if len(tokens) < window_size + 10:\n",
    "    repeats = math.ceil((window_size + 10) / max(1, len(tokens)))\n",
    "    tokens = tokens * repeats\n",
    "    print(f\"Text was short; repeated tokens {repeats} times -> new length {len(tokens)}\")"
   ],
   "id": "de40945dfb5c1ab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 89701\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:41:05.866461Z",
     "start_time": "2026-02-21T11:41:04.540703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build vocabulary\n",
    "counter = Counter(tokens)\n",
    "vocab_tokens = [w for w, c in counter.items() if c >= min_freq]\n",
    "vocab_tokens.sort(key=lambda w: (-counter[w], w))\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "START = \"<START>\"\n",
    "\n",
    "itos = [PAD, UNK, START] + vocab_tokens\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def tok2idx(tok):\n",
    "    return stoi.get(tok, stoi[UNK])\n",
    "\n",
    "seq_len = window_size - 1\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(0, len(tokens) - seq_len):\n",
    "    inp = tokens[i : i + seq_len]\n",
    "    targ = tokens[i + seq_len]\n",
    "    inputs.append([tok2idx(t) for t in inp])\n",
    "    targets.append(tok2idx(targ))\n",
    "\n",
    "print(\"Number of sequences:\", len(inputs))"
   ],
   "id": "c79ee9e0560f1151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7362\n",
      "Number of sequences: 89602\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:41:05.893878Z",
     "start_time": "2026-02-21T11:41:05.885276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextSeqDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "dataset = TextSeqDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "id": "41b1992a0267d793",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:41:08.312180Z",
     "start_time": "2026-02-21T11:41:05.922593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RNNGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.gru(emb, hidden)\n",
    "        out = out[:, -1, :]\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "model = RNNGenerator(vocab_size, embedding_dim, hidden_size, num_layers=num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(model)"
   ],
   "id": "ca6e68507d4b9fba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNGenerator(\n",
      "  (embedding): Embedding(7362, 128, padding_idx=0)\n",
      "  (gru): GRU(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=7362, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "e209c5c6a1d780fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T12:57:30.932714Z",
     "start_time": "2026-02-21T11:41:08.540215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch}/{epochs} — loss: {loss:.4f}\")"
   ],
   "id": "2018751a3f29e82d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 — loss: 6.5189\n",
      "Epoch 2/8 — loss: 5.4833\n",
      "Epoch 3/8 — loss: 4.8622\n",
      "Epoch 4/8 — loss: 4.3158\n",
      "Epoch 5/8 — loss: 3.7544\n",
      "Epoch 6/8 — loss: 3.1887\n",
      "Epoch 7/8 — loss: 2.6529\n",
      "Epoch 8/8 — loss: 2.1767\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T12:57:36.009341Z",
     "start_time": "2026-02-21T12:57:32.824705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model, seed_text, max_words=120, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = clean_and_tokenize(seed_text)\n",
    "    while len(words) < seq_len:\n",
    "        words = [START] + words\n",
    "    generated = words[:]\n",
    "    hidden = None\n",
    "    for _ in range(max_words):\n",
    "        input_seq = generated[-seq_len:]\n",
    "        idxs = torch.tensor([tok2idx(w) for w in input_seq], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, hidden = model(idxs, hidden)\n",
    "            logits = logits.squeeze(0) / (temperature if temperature>0 else 1.0)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_word = itos[next_idx] if next_idx < len(itos) else UNK\n",
    "            generated.append(next_word)\n",
    "    out = []\n",
    "    for token in generated:\n",
    "        if re.match(r\"^\\w+$\", token):  # word\n",
    "            if out:\n",
    "                out.append(\" \")\n",
    "            out.append(token)\n",
    "        else:  # punctuation\n",
    "            out.append(token)\n",
    "    return \"\".join(out)\n",
    "\n",
    "seed = \"the monster\"\n",
    "print(\"Generated sample:\\n\")\n",
    "print(generate_text(model, seed, max_words=120, temperature=1.0))"
   ],
   "id": "934cd3b1ca10c0c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample:\n",
      "\n",
      "<START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START><START> the monster both sight as their home added. during my residence days scarcely conjectured when beaufort did not a little wants might perform a blessing to visit them; but i followed that would pass my words and permit me with pleasure.[ o night; and when he appeared to betray them in his illness, and to some sudden from which the end of the town of mankind could bestow without the practical danger of science. the raising of two days awoke with the idea finest both, and they conjectured still more act of the news; the darkness of a pistol- satisfaction, according by an especial wind among the northern bed; and\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model",
   "id": "d35af3eec038cdef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T12:57:36.126730Z",
     "start_time": "2026-02-21T12:57:36.023217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"stoi\": stoi,\n",
    "    \"itos\": itos,\n",
    "    \"config\": {\n",
    "        \"window_size\": window_size,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers\n",
    "    }\n",
    "}, \"rnn_text_gen.pth\")\n",
    "print(\"Saved model to rnn_text_gen.pth\")"
   ],
   "id": "a5e5a3ad79070acf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to rnn_text_gen.pth\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
