{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web scrapping",
   "id": "2bceddfd311b6ad8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-21T22:47:57.278759Z",
     "start_time": "2026-02-21T22:47:56.860967Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:10.077323Z",
     "start_time": "2026-02-21T22:48:05.684361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/84/pg84-images.html\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "text = soup.get_text()\n",
    "\n",
    "start_marker = \"To Mrs. Saville, England.\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\"\n",
    "\n",
    "start = text.find(start_marker)\n",
    "end = text.find(end_marker)\n",
    "\n",
    "if start != -1:\n",
    "    text = text[start + len(start_marker):]\n",
    "\n",
    "if end != -1:\n",
    "    text = text[:end]\n",
    "\n",
    "print(text[:2000])"
   ],
   "id": "18125d47b4ae82f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\r\n",
      "St. Petersburgh, Dec. 11th, 17—.\r\n",
      "\n",
      "\r\n",
      "You will rejoice to hear that no disaster has accompanied the commencement of\r\n",
      "an enterprise which you have regarded with such evil forebodings. I arrived\r\n",
      "here yesterday, and my first task is to assure my dear sister of my welfare and\r\n",
      "increasing confidence in the success of my undertaking.\r\n",
      "\n",
      "\r\n",
      "I am already far north of London, and as I walk in the streets of Petersburgh,\r\n",
      "I feel a cold northern breeze play upon my cheeks, which braces my nerves and\r\n",
      "fills me with delight. Do you understand this feeling? This breeze, which has\r\n",
      "travelled from the regions towards which I am advancing, gives me a foretaste\r\n",
      "of those icy climes. Inspirited by this wind of promise, my daydreams become\r\n",
      "more fervent and vivid. I try in vain to be persuaded that the pole is the seat\r\n",
      "of frost and desolation; it ever presents itself to my imagination as the\r\n",
      "region of beauty and delight. There, Margaret, the sun is for ever visible, its\r\n",
      "broad disk just skirting the horizon and diffusing a perpetual splendour.\r\n",
      "There—for with your leave, my sister, I will put some trust in preceding\r\n",
      "navigators—there snow and frost are banished; and, sailing over a calm sea, we\r\n",
      "may be wafted to a land surpassing in wonders and in beauty every region\r\n",
      "hitherto discovered on the habitable globe. Its productions and features may be\r\n",
      "without example, as the phenomena of the heavenly bodies undoubtedly are in\r\n",
      "those undiscovered solitudes. What may not be expected in a country of eternal\r\n",
      "light? I may there discover the wondrous power which attracts the needle and\r\n",
      "may regulate a thousand celestial observations that require only this voyage to\r\n",
      "render their seeming eccentricities consistent for ever. I shall satiate my\r\n",
      "ardent curiosity with the sight of a part of the world never before visited,\r\n",
      "and may tread a land never before imprinted by the foot of man. These are my\r\n",
      "enticements, and they are sufficient to conquer all fear of danger or death and\r\n",
      "to induce me to \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Preprocessing",
   "id": "f173af4a57d10618"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:22.345262Z",
     "start_time": "2026-02-21T22:48:22.312798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = re.sub(r'\\n\\s*(letter|chapter)\\s+\\d+\\s*\\n', '\\n', text, flags=re.IGNORECASE)\n",
    "\n",
    "text = re.sub(\n",
    "    r'(^|\\n)[A-Za-z.\\- ]*,?\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\.?\\,?\\s*(\\d{1,2}(st|nd|rd|th)?)?,?\\s*(17—\\.)?\\s*',\n",
    "    r'\\1',\n",
    "    text\n",
    ")\n",
    "\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text = text.strip()\n",
    "\n",
    "print(text[:2000])"
   ],
   "id": "7a1c4bf3ed23f847",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. i arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking. i am already far north of london, and as i walk in the streets of petersburgh, i feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. do you understand this feeling? this breeze, which has travelled from the regions towards which i am advancing, gives me a foretaste of those icy climes. inspirited by this wind of promise, my daydreams become more fervent and vivid. i try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the garet, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. there—for with your leave, my sister, i will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe. its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. what may not be expected in a country of eternal light? i may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever. i shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man. these are my enticements, and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:34.183676Z",
     "start_time": "2026-02-21T22:48:34.166204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = re.sub(r'([.!?])', r'\\1 <END>', text)\n",
    "\n",
    "text = re.sub(r'[^\\w\\s<>]', '', text)\n",
    "\n",
    "print(text[:2000])"
   ],
   "id": "2971acf296c72745",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings <END> i arrived here yesterday and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking <END> i am already far north of london and as i walk in the streets of petersburgh i feel a cold northern breeze play upon my cheeks which braces my nerves and fills me with delight <END> do you understand this feeling <END> this breeze which has travelled from the regions towards which i am advancing gives me a foretaste of those icy climes <END> inspirited by this wind of promise my daydreams become more fervent and vivid <END> i try in vain to be persuaded that the pole is the seat of frost and desolation it ever presents itself to my imagination as the garet the sun is for ever visible its broad disk just skirting the horizon and diffusing a perpetual splendour <END> therefor with your leave my sister i will put some trust in preceding navigatorsthere snow and frost are banished and sailing over a calm sea we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe <END> its productions and features may be without example as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes <END> what may not be expected in a country of eternal light <END> i may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever <END> i shall satiate my ardent curiosity with the sight of a part of the world never before visited and may tread a land never before imprinted by the foot of man <END> these are my enticements and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he emb\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization",
   "id": "b56a05c228661620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:36.194362Z",
     "start_time": "2026-02-21T22:48:36.186156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = text.split()\n",
    "print(tokens[:20])\n",
    "print(\"Total tokens:\", len(tokens))"
   ],
   "id": "43e4613e8e5f6947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with']\n",
      "Total tokens: 78430\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:38.087430Z",
     "start_time": "2026-02-21T22:48:38.071986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = sorted(set(tokens))\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)"
   ],
   "id": "f9efe60196ed845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7188\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:41.192254Z",
     "start_time": "2026-02-21T22:48:41.181560Z"
    }
   },
   "cell_type": "code",
   "source": "encoded = [word_to_idx[word] for word in tokens]",
   "id": "31f05c43d2009dae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:42.595618Z",
     "start_time": "2026-02-21T22:48:42.435661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 100\n",
    "sequence_length = window_size - 1\n",
    "\n",
    "data = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(encoded) - window_size):\n",
    "    data.append(encoded[i:i + sequence_length])\n",
    "    targets.append(encoded[i + sequence_length])\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(len(targets))"
   ],
   "id": "a903aaaeff8b01b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78330\n",
      "99\n",
      "78330\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "8475cc673e708a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:48:49.847963Z",
     "start_time": "2026-02-21T22:48:44.603281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(MyRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "\n",
    "        last_output = output[:, -1, :]\n",
    "\n",
    "        out = self.fc(last_output)\n",
    "        return out"
   ],
   "id": "f974d8286159ed97",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:50:39.897277Z",
     "start_time": "2026-02-21T22:50:39.832420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = MyRNN(vocab_size, embed_dim=128, hidden_dim=256).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "4aa04b2d67ac7047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:50:41.939409Z",
     "start_time": "2026-02-21T22:50:41.388777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor(data, dtype=torch.long)\n",
    "y = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "print(X.shape, y.shape)"
   ],
   "id": "c3f8152efcc19c5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78330, 99]) torch.Size([78330])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:50:43.561500Z",
     "start_time": "2026-02-21T22:50:43.556104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "id": "896f053309c1f14b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:55:37.673678Z",
     "start_time": "2026-02-21T22:53:09.928946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_X, batch_y in dataloader:\n",
    "\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")"
   ],
   "id": "24716c23a0a09c53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.7925\n",
      "Epoch [2/30], Loss: 1.6252\n",
      "Epoch [3/30], Loss: 1.4826\n",
      "Epoch [4/30], Loss: 1.3626\n",
      "Epoch [5/30], Loss: 1.2524\n",
      "Epoch [6/30], Loss: 1.1630\n",
      "Epoch [7/30], Loss: 1.0799\n",
      "Epoch [8/30], Loss: 1.0147\n",
      "Epoch [9/30], Loss: 0.9500\n",
      "Epoch [10/30], Loss: 0.9024\n",
      "Epoch [11/30], Loss: 0.8536\n",
      "Epoch [12/30], Loss: 0.8162\n",
      "Epoch [13/30], Loss: 0.7818\n",
      "Epoch [14/30], Loss: 0.7491\n",
      "Epoch [15/30], Loss: 0.7231\n",
      "Epoch [16/30], Loss: 0.7009\n",
      "Epoch [17/30], Loss: 0.6856\n",
      "Epoch [18/30], Loss: 0.6644\n",
      "Epoch [19/30], Loss: 0.6515\n",
      "Epoch [20/30], Loss: 0.6443\n",
      "Epoch [21/30], Loss: 0.6204\n",
      "Epoch [22/30], Loss: 0.6262\n",
      "Epoch [23/30], Loss: 0.6130\n",
      "Epoch [24/30], Loss: 0.6096\n",
      "Epoch [25/30], Loss: 0.6038\n",
      "Epoch [26/30], Loss: 0.6086\n",
      "Epoch [27/30], Loss: 0.6030\n",
      "Epoch [28/30], Loss: 0.6049\n",
      "Epoch [29/30], Loss: 0.6032\n",
      "Epoch [30/30], Loss: 0.6053\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Next Word Generation",
   "id": "a4d2ff3732ed42cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:07:04.766110Z",
     "start_time": "2026-02-21T23:07:04.759270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text(model, seed_text, length, device):\n",
    "    model.eval()\n",
    "\n",
    "    words = seed_text.lower().split()\n",
    "    words = [w for w in words if w in word_to_idx]\n",
    "\n",
    "    if len(words) == 0:\n",
    "        return \"Seed words not found in vocabulary.\"\n",
    "\n",
    "    input_seq = [word_to_idx[w] for w in words]\n",
    "\n",
    "    for _ in range(length):\n",
    "\n",
    "        if len(input_seq) < 99:\n",
    "            padded = [0]*(99 - len(input_seq)) + input_seq\n",
    "        else:\n",
    "            padded = input_seq[-99:]\n",
    "\n",
    "        input_tensor = torch.tensor([padded], dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()\n",
    "            predicted_word = idx_to_word[predicted_idx]\n",
    "\n",
    "        if predicted_word == \"<END>\":\n",
    "            words.append(\".\")\n",
    "            break\n",
    "\n",
    "        words.append(predicted_word)\n",
    "        input_seq.append(predicted_idx)\n",
    "\n",
    "    return \" \".join(words)"
   ],
   "id": "56cdb3b6342e29a3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:07:05.603074Z",
     "start_time": "2026-02-21T23:07:05.437259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed_text = [\"i will explore\", \"i world like to know\", \"my dream was\", \"i felt a strange\", \"at night\"]\n",
    "\n",
    "for i in range(len(seed_text)):\n",
    "    test = generate_text(model, seed_text[i], length=10, device=device)\n",
    "    print(f\"Sentence {i+1}:\", test)"
   ],
   "id": "fb77dedddd1b9e0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: i will explore into fear and at home the greatest affection and even\n",
      "Sentence 2: i world like to know the shore i might have put into the harbour ignorant\n",
      "Sentence 3: my dream was as a child there to another in mutual bonds .\n",
      "Sentence 4: i felt a strange nature pressed upon me .\n",
      "Sentence 5: at night on her head passing with the experience of safie who\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
