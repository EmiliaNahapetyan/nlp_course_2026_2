{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CElx0sa2H8le",
        "outputId": "0043bbbb-5232-46b5-e9a8-abd8d9909a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/cache/epub/84/pg84-images.html\"\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "text = soup.get_text()\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub(r'\\s+', ' ', text)"
      ],
      "metadata": {
        "id": "VIZ5wdXBJbdN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "print(\"Total words:\", len(tokens))"
      ],
      "metadata": {
        "id": "tgYwEvbwJiWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df24b3bf-0696-4983-c53c-255aa14556a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 78532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary\n",
        "vocab = list(set(tokens))\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx2word = {i: word for word, i in word2idx.items()}\n",
        "\n",
        "encoded = [word2idx[word] for word in tokens]\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "77CzXvvTJrz5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 100\n",
        "sequence_length = 99\n",
        "\n",
        "data = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(encoded) - window_size):\n",
        "    data.append(encoded[i:i+sequence_length])\n",
        "    targets.append(encoded[i+sequence_length])\n",
        "\n",
        "print(len(data))\n",
        "print(len(data[0]))  # should be 99"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2srMUgzNhBO",
        "outputId": "29ddb37f-e28d-4fb0-d3d0-76399bce05e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78432\n",
            "99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(data, dtype=torch.long)\n",
        "y = torch.tensor(targets, dtype=torch.long)"
      ],
      "metadata": {
        "id": "LXfeLuZhNjER"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, 64)\n",
        "        self.rnn = nn.RNN(64, 128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, hidden = self.rnn(x)\n",
        "        last_output = output[:, -1, :]\n",
        "        out = self.fc(last_output)\n",
        "        return out\n",
        "\n",
        "model = SimpleRNN(vocab_size)"
      ],
      "metadata": {
        "id": "bUUr3p9yNlGl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 3\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        inputs = X[i:i+batch_size]\n",
        "        labels = y[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(\"Epoch:\", epoch+1, \"Loss:\", total_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLnRf3_MNpMy",
        "outputId": "68fb7d9a-57de-41d6-d76e-2c4c37e46c2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 4019.318055152893\n",
            "Epoch: 2 Loss: 3552.2107486724854\n",
            "Epoch: 3 Loss: 3343.4099159240723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, length=100):\n",
        "    model.eval()\n",
        "\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    for _ in range(length):\n",
        "        input_ids = [word2idx.get(w, 0) for w in words[-99:]]\n",
        "        input_tensor = torch.tensor([input_ids], dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            predicted_id = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        predicted_word = idx2word[predicted_id]\n",
        "        words.append(predicted_word)\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "print(generate_text(\"i saw the\", 120))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkE_QcfxNrey",
        "outputId": "1f01fbda-76e4-4168-c08f-19d7297c1699"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i saw the work of the project gutenberg literary archive foundation or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works or any other project gutenberg electronic works\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5U-1hVH4OysI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}